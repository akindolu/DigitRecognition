{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Digits Recognizer\n",
    "\n",
    "This notebook shows the method of developing a digits recognizer for the Digits Recognizer Kaggle Competition\n",
    "\n",
    "First, we import all the classes and functions we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is disabled, cuDNN not available)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "import theano\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import random\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set the random number seed, to ensure reprducibility of our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the required data from file.\n",
    "\n",
    "The train data is located in \"train.csv\", while the test data is located in \"test.csv\". The train data includes labels, while the test data does not include labels. The prediction labels obtained form the test data is uploaded on kaggle to be scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the three data sets is: \n",
      "(42000, 784)\n",
      "(42000,)\n",
      "(28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "y_train = dataset[[0]].values.ravel()\n",
    "X_train = dataset.iloc[:,1:].values\n",
    "X_test = pd.read_csv(\"test.csv\").values\n",
    "num_pixels = X_train.shape[1]\n",
    "\n",
    "print(\"The shape of the three data sets is: \")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (42000, 784)\n",
      "The shape of X_test is: (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "print(\"The shape of X_train is: \" + str(X_train.shape))\n",
    "print(\"The shape of X_test is: \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now scale the features from 0 - 255 to 0 - 1, and we one hot encode the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a simple network with dropout\n",
    "\n",
    "We add dropout after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_pixels*2, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_pixels*4, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_pixels*2, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit over 10 epochs with a batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/20\n",
      "26s - loss: 0.6970 - acc: 0.8445 - val_loss: 0.1703 - val_acc: 0.9526\n",
      "Epoch 2/20\n",
      "26s - loss: 0.1954 - acc: 0.9430 - val_loss: 0.1373 - val_acc: 0.9574\n",
      "Epoch 3/20\n",
      "26s - loss: 0.1505 - acc: 0.9574 - val_loss: 0.1328 - val_acc: 0.9617\n",
      "Epoch 4/20\n",
      "26s - loss: 0.1329 - acc: 0.9624 - val_loss: 0.1410 - val_acc: 0.9638\n",
      "Epoch 5/20\n",
      "26s - loss: 0.1102 - acc: 0.9694 - val_loss: 0.1311 - val_acc: 0.9671\n",
      "Epoch 6/20\n",
      "27s - loss: 0.1015 - acc: 0.9721 - val_loss: 0.1043 - val_acc: 0.9740\n",
      "Epoch 7/20\n",
      "26s - loss: 0.0908 - acc: 0.9744 - val_loss: 0.1259 - val_acc: 0.9705\n",
      "Epoch 8/20\n",
      "26s - loss: 0.0898 - acc: 0.9754 - val_loss: 0.1250 - val_acc: 0.9698\n",
      "Epoch 9/20\n",
      "26s - loss: 0.0805 - acc: 0.9780 - val_loss: 0.1074 - val_acc: 0.9733\n",
      "Epoch 10/20\n",
      "26s - loss: 0.0815 - acc: 0.9785 - val_loss: 0.1103 - val_acc: 0.9745\n",
      "Epoch 11/20\n",
      "26s - loss: 0.0865 - acc: 0.9783 - val_loss: 0.1556 - val_acc: 0.9700\n",
      "Epoch 12/20\n",
      "26s - loss: 0.0869 - acc: 0.9792 - val_loss: 0.1145 - val_acc: 0.9752\n",
      "Epoch 13/20\n",
      "26s - loss: 0.0724 - acc: 0.9815 - val_loss: 0.1355 - val_acc: 0.9688\n",
      "Epoch 14/20\n",
      "26s - loss: 0.0766 - acc: 0.9817 - val_loss: 0.1188 - val_acc: 0.9731\n",
      "Epoch 15/20\n",
      "26s - loss: 0.0657 - acc: 0.9835 - val_loss: 0.1506 - val_acc: 0.9707\n",
      "Epoch 16/20\n",
      "26s - loss: 0.0665 - acc: 0.9835 - val_loss: 0.1133 - val_acc: 0.9769\n",
      "Epoch 17/20\n",
      "26s - loss: 0.0625 - acc: 0.9845 - val_loss: 0.1432 - val_acc: 0.9740\n",
      "Epoch 18/20\n",
      "26s - loss: 0.0609 - acc: 0.9849 - val_loss: 0.1275 - val_acc: 0.9764\n",
      "Epoch 19/20\n",
      "26s - loss: 0.0624 - acc: 0.9853 - val_loss: 0.1324 - val_acc: 0.9769\n",
      "Epoch 20/20\n",
      "26s - loss: 0.0625 - acc: 0.9855 - val_loss: 0.1693 - val_acc: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57b73e3650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_split = 0.1, nb_epoch=20, batch_size=100, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predict and write the prediction results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
